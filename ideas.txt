Algoritmo Top-Down

<<<<<<< HEAD
ESTRUCTURA GENERAL DEL ALGORITMO EN BASE A UN ANÁLISIS TOP-DOWN

    CREATE
        dada una carpeta de archivos pdf hacer:
        para cada archivo pdf "P" dentro de la carpeta hacer:

            crear un TRIE para P
            crear una HASH para P
            FUNCIÓN_1: separar el texto en palabras
            FUNCION_2: discernir su naturaleza: es __clasificable__ o no
            si la palabra es __clasificable__ hacer:
            (si no lo es, pasar a la próxima palabra)
                tomar la palabra que es __clasificable__ y hacer:
                    FUNCION_3: desnudar la palabra
                    FUNCION_4: buscarle una __palabraasociada__ en el TRIE, si no tiene, agregarla al TRIE
                    FUNCION_5: en base a lo entregado por FUNCION_4, actualizar valor en HASH
            >>> al final, tendríamos tantas TRIES y HASHES como pdf's
=======
Create <path>

    Para cada archivo PDF en la carpeta:
        Crear un TRIE y un HASH para el PDF.
        Función 1: Separar el texto en palabras.
        Función 2: Determinar si la palabra es clasificable.
        Si la palabra es clasificable:
            Función 3: Normalizar la palabra.
            Función 4: Buscar palabra asociada en el TRIE. Si no existe, agregarla.
            Función 5: Actualizar el valor en el HASH según lo entregado por la Función 4.
        Resultado: TRIES y HASHES por cada PDF.

Search <texto>

    Criterios de Semántica:
        Base de datos de palabras clave.
    Procesar texto de búsqueda:
        Usar Funciones 2, 3, y 4.
        Comparar similitud con cada HASH.
    Resultados:
        Lista de % de correspondencia.
        Ordenar y presentar la lista.
>>>>>>> 5f22b47ecae3de7d636246e21326dea950592ea7

Funciones

    Función 1: Separar palabras usando .split(' ').
    Función 2: Determinar si una palabra es clasificable.
    Función 3: Normalizar palabras:
        Quitar prefijos.
        Convertir a minúsculas.
        Quitar tildes.
    Función 4: Buscar o agregar palabra en TRIE:
        Si se encuentra una palabra asociada, devolverla.
        Si no, agregar la palabra y devolverla.
    Función 5: Actualizar HASH:
        Si la palabra existe, actualizar su valor.
        Si no, agregarla con valor inicial.

Conceptos

    Clasificable: No es un artículo, conector, o proposición.
    Palabra asociada: Una palabra que contiene al menos la mitad de los caracteres de otra en orden.

Ideas

    Usar with para manejar archivos.
    Utilizar yaml si es necesario.
    Serialización con pickle.dump.

Procedimiento Final

<<<<<<< HEAD
Dada una lista de palabras pertenecientes a un documento, necesitamos una función con esta especificación:

getFreqHash(lst)

Entrada: lista Python de palabras (strings)
Proceso: se crea una tabla hash que use linear probing, esta tabla debe tener por slots el par (key, value) donde "key" es el string y "value" es la cantidad de veces que ha aparecido esa palabra dividido entre el número total de palabras (n)
Salida: la tabla hash resultante





FUNCION_1
    create_word_list(pdf_i)
    >> tomo un pdf y hace la separación correspondiente en palabras, esta las va agregando a una lista
    << retorna la lista de palabras correspondientes


FUNCION_2
    create_empty_hash(hash_general, lista_palabras_pdf_i)
    >> toma y recorre una lista_palabras_pdf_i (FUNCION_1), y agrega cada palabra a un diccionario "referencia" con un
    value vacío
    << devuelve la hash_general (un diccionario "referencia")


    en el segundo barrido actualizamos los values con el tf de cada palabra de cada documento, a la par se hace el idf general
    cuando termina el segundo barrido, se actualizan los values de los vectores tf para que tenga el tf-idf final (se multiplica por idf básicamente)


FUNCION_3
    create_tf(lista_palabras_pdf_i)
    >> toma lista_palabras_pdf_i (FUNCION_1), recorre y hace el tf tal como ya lo tenemos hecho: es decir, también hace
    un contador general y luego divide el value entre este valor
    >> retorna la hash-tf_i (particular)


FUNCION_4
    create_idf(hash-tf_i, idf_general)

    FUNCION_5
        doc_counter(hash-tf_i, idf_general)
        >> toma el hash particular y agrega un contador al idf_general, recordar, que con que la palabra este, es suficiente
         para ese documento
        << devuelve, opcionalmente (por ser de referencia), el idf_general

    FUNCION_6
        una vez se hayan barrido todos los documentos, y hechos los contadores, hacer:
        apply_idf(idf_general)
        >> para cada key_value del idf general, hacer: log2(N/value), donde N es el tamaño del corpus
        << devolver el idf_general


FUNCION_7
    una vez se tengan todos los tfs, y el idf general, se procede a la multiplicación de ambos valores para que cada
    documento tenga sus vectores completos en dimensión
    esto se hace documento por documento, ie, tf_i por tf_i


    multiply_tf_idf(tf_i, idf_general)
    >> para cada key-value del tf_i, hace el producto (solo el producto), con el idf
    << devuelve el weight_vector_i del documento_i

    se van agregando estos vectores a una lista

FUNCION_8
hacer el pickle una vez se terminan los hashes (vectores) originales, también con una hash de values vacía para tener de referencia y usarla con el texto del search







con el texto del search, hacer lo mismo (de crear el tf-idf), si una palabra no está en la hash de referencia del corpus, agregarla a cada hash (vector) de cada documento

hacer la similitud entre la hash del texto y cada uno de los vectores de los documentos (ahora, hay coherencia de componentes y significado)
más cerca de 1, mayor similitud entre documentos

ordenar e imprimir






hacer la semántica para quitar plurales, tratando de no romper las palabras
si el trie funciona, usarlo para un mejor refinado




=======
    Crear TRIES y HASHES para cada PDF.
    Procesar texto de búsqueda y calcular similitud.
    Ordenar resultados y presentarlos.
>>>>>>> 5f22b47ecae3de7d636246e21326dea950592ea7
